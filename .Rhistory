predict.train.QDA <- predict(train.QDA, train.data)
train.QDA.error <- mean(predict.train.QDA$class != train.data$Species)
train.QDA.error
```
```{r}
test.QDA <- lda(Species ~ ., data = iris, CV = T)
test.QDA.error <- mean(test.QDA$class != iris$Species)
test.QDA.error
```
```{r}
#Construct a data table or training and test error rates
KNN.df <- error.df[4,2:3]
names(KNN.df) <- c("train.error", "test.error")
LDA.df <- data.frame(train.error = train.LDA.error, test.error = test.LDA.error)
QDA.df <- data.frame(train.error = train.QDA.error, test.error =test.QDA.error)
model.errors <- rbind(KNN.df, LDA.df, QDA.df)
model.errors <- data.frame(model = c("KNN", "LDA", "QDA"), model.errors)
error.plot <- ggplot(model.errors, aes(x = train.error, y = test.error, color = model))
error.plot <- error.plot + geom_point(size = 10) + expand_limits(x = 0, y = 0)
error.plot
```
Based on error rates, we see that QDA best model. Minimized training error, tied for best test error with LDA. Possible LDA may be better with some samples. Use bootstrap to confirm our hypothesis.
## Bootstrap Method
```{r}
#Create empty vectors to append error rate in each iteration of the for loop
knnErrors = vector()
ldaErrors = vector()
qdaErrors = vector()
set.seed(131)
#Run each model 100 times
for (i in 1:100){
#Create new dataset of same size with random sample of iris data
sampleIndex <- sample(1:nrow(iris), size = nrow(iris), replace = TRUE)
sampleData <- iris[sampleIndex, ]
# Fit the three models to the new sample data
KNNfit <- knn.cv(train = sampleData[, 1:4], cl = sampleData[, 5], k = 4)
LDAfit <- lda(Species ~ ., data = sampleData, CV = T)
QDAfit <- qda(Species ~ ., data = sampleData, CV = T)
# Compute error rates for each iteration
knnError <- mean(KNNfit != sampleData$Species)
ldaError <- mean(LDAfit$class != sampleData$Species)
qdaError <- mean(QDAfit$class != sampleData$Species)
# Save error rates for each iteration
knnErrors[i] = knnError
ldaErrors[i] = ldaError
qdaErrors[i] = qdaError
}
# Aggregate results
knnMean <- mean(knnErrors)
knnVar <- var(knnErrors)
ldaMean <- mean(ldaErrors)
ldaVar <- var(ldaErrors)
qdaMean <- mean(qdaErrors)
qdaVar <- var(qdaErrors)
meanKNN <- paste("Mean KNN Error Rate =", knnMean)
varKNN <- paste("Variance of KNN Error Rate =", knnVar)
meanLDA <- paste("Mean LDA Error Rate =", ldaMean)
varLDA <- paste("Variance of LDA Error Rate =", ldaVar)
meanQDA <- paste("Mean QDA Error Rate =", qdaMean)
varQDA <- paste("Variance of QDA Error Rate =", qdaVar)
cat(paste(meanKNN,varKNN,meanLDA,varLDA,meanQDA,varQDA, sep = '\n'))
```
error.plot
model.errors
---
title: "Iris Plant Classification Using Machine Learning"
author: "Chris Meade"
date: "11/6/2016"
output: pdf_document
---
```{r, echo=F, message=F, warning=F}
require(ggplot2)
require(GGally)
require(ggthemes)
require(stats)
require(MASS)
require(class)
require(stats)
require(xtable)
```
# 1. Introduction
## About the Data Set
The iris data set contains 150 observations of 5 variables. The species of the plant, `Species` is the response variable to be predicted from the other four variables: `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`.
## Importing the Data
```{r, message=FALSE}
data(iris)
iris = as.data.frame(iris)
```
## Exploratory Data Analysis
We first begin by examing how the variables interact with one another
```{r}
scatterPlot <- ggpairs(data=iris,
columns=1:4,
upper = list(continuous = wrap("smooth")),
lower = list(continuous = wrap("points"), combo = wrap("dot")),
aes(color = Species),
axisLabels = "internal",
title = "Iris Scatterplot Matrix")
scatterPlot
scatterPlot2 <- ggpairs(data=iris,
columns=1:4,
upper = "blank",
lower = "blank",
aes(color = Species),
title = "Iris Densitys")
scatterPlot2
```
We make the following observations that guide our analysis:
1. There appears to be clear separation of the observations by species when the predicator variables are compared. These makes this data set a prime candidate for machine learning prediction.
2. `Sepal.Length`, `Petal.Length`, and `Petal.Width` are highly correlated. `Sepal.Width` is not.
## Split the Data into a Training Set and Test Set
```{r, message = FALSE}
# Separate predictor variables from Species
X.iris = iris[, c('Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width')]
Y.iris = iris[, 'Species']
# Reserve 30% of the data for a test set
train.percent <- 0.70
train.number <- train.percent * nrow(iris)
train.indices <- sample.int(nrow(iris), train.number)
# Extract the training set using our train indices
X.train <- X.iris[train.indices,]
Y.train <- Y.iris[train.indices]
train.data <- iris[train.indices,]
# Get the test set from the rest
X.test <- X.iris[-c(train.indices),]
Y.test <- Y.iris[-c(train.indices)]
test.data <- iris[-c(train.indices),]
```
# Model Construction
## K-Nearest Neightbors
Description of knn
```{r}
# Build he Model for k=1:15
#Calculate Training Error Rates
train.KNN.error = rep(0,15)
k = 1:15
for(i in k ){
model <- knn(X.train,
X.train,
Y.train,
k = i)
train.KNN.error[i] <- mean(model != Y.train)
}
#Calculate Test Error Rates
test.KNN.error = rep(0,15)
k = 1:15
for(i in k ){
model <- knn(X.train,
X.test,
Y.train,
k = i)
test.KNN.error[i] <- mean(model != Y.test)
}
error.df <- data.frame(k, train.KNN.error, test.KNN.error)
ggplot(error.df) +
geom_path(aes(k, train.KNN.error, color = 'red')) +
geom_path(aes(k, test.KNN.error, color = 'blue'))+
ggtitle("Train (red) & Test (blue) Error Rates vs. K")
```
Based on the graph on error rates, `k = 4` appears to provide a good compromise between not overfitting the data and providing accurate predictions of the test set. So with our KNN model, we have the following performance:
Model Analysis
```{r}
# Error Rates
error.df[4,]
#ADD More shit
```
## Linear Discriminant Analysis
Describe this shit
```{r}
# Construct the Model
train.LDA <- lda(Species ~ ., data = train.data)
# Predict the Species class of the training set
predict.train.LDA <- predict(train.LDA, train.data)
# Calculate Training Test Rate
train.LDA.error<- mean(predict.train.LDA$class != train.data$Species)
train.LDA.error
```
```{r}
# Use Leave-one-out cross-validation to calculate test error rate
test.LDA <- lda(Species ~ ., data = iris, CV = T)
test.LDA.error <- mean(test.LDA$class != iris$Species)
test.LDA.error
```
LDA yields a test error rate of training error rate of $0.01904762$ and test error rate of $0.02$.
## QDA
Talk about QDA
```{r}
train.QDA <- qda(Species ~ ., data = train.data)
predict.train.QDA <- predict(train.QDA, train.data)
train.QDA.error <- mean(predict.train.QDA$class != train.data$Species)
train.QDA.error
```
```{r}
test.QDA <- lda(Species ~ ., data = iris, CV = T)
test.QDA.error <- mean(test.QDA$class != iris$Species)
test.QDA.error
```
```{r}
#Construct a data table or training and test error rates
KNN.df <- error.df[4,2:3]
names(KNN.df) <- c("train.error", "test.error")
LDA.df <- data.frame(train.error = train.LDA.error, test.error = test.LDA.error)
QDA.df <- data.frame(train.error = train.QDA.error, test.error =test.QDA.error)
model.errors <- rbind(KNN.df, LDA.df, QDA.df)
model.errors <- data.frame(model = c("KNN", "LDA", "QDA"), model.errors)
error.plot <- ggplot(model.errors, aes(x = train.error, y = test.error, color = model))
error.plot <- error.plot + geom_point(size = 10) + expand_limits(x = 0, y = 0)
error.plot
```
Based on error rates, we see that QDA best model. Minimized training error, tied for best test error with LDA. Possible LDA may be better with some samples. Use bootstrap to confirm our hypothesis.
## Bootstrap Method
```{r}
#Create empty vectors to append error rate in each iteration of the for loop
knnErrors = vector()
ldaErrors = vector()
qdaErrors = vector()
set.seed(131)
#Run each model 100 times
for (i in 1:100){
#Create new dataset of same size with random sample of iris data
sampleIndex <- sample(1:nrow(iris), size = nrow(iris), replace = TRUE)
sampleData <- iris[sampleIndex, ]
# Fit the three models to the new sample data
KNNfit <- knn.cv(train = sampleData[, 1:4], cl = sampleData[, 5], k = 4)
LDAfit <- lda(Species ~ ., data = sampleData, CV = T)
QDAfit <- qda(Species ~ ., data = sampleData, CV = T)
# Compute error rates for each iteration
knnError <- mean(KNNfit != sampleData$Species)
ldaError <- mean(LDAfit$class != sampleData$Species)
qdaError <- mean(QDAfit$class != sampleData$Species)
# Save error rates for each iteration
knnErrors[i] = knnError
ldaErrors[i] = ldaError
qdaErrors[i] = qdaError
}
# Aggregate results
knnMean <- mean(knnErrors)
knnVar <- var(knnErrors)
ldaMean <- mean(ldaErrors)
ldaVar <- var(ldaErrors)
qdaMean <- mean(qdaErrors)
qdaVar <- var(qdaErrors)
meanKNN <- paste("Mean KNN Error Rate =", knnMean)
varKNN <- paste("Variance of KNN Error Rate =", knnVar)
meanLDA <- paste("Mean LDA Error Rate =", ldaMean)
varLDA <- paste("Variance of LDA Error Rate =", ldaVar)
meanQDA <- paste("Mean QDA Error Rate =", qdaMean)
varQDA <- paste("Variance of QDA Error Rate =", qdaVar)
cat(paste(meanKNN,varKNN,meanLDA,varLDA,meanQDA,varQDA, sep = '\n'))
```
install.packages("twittR")
install.packages("twitteR")
install.packages("OAuth")
install.packages("ROAuth")
install.packages("httr")
library(httr)
library("twitteR", lib.loc="~/Library/R/3.2/library")
library("ROAuth", lib.loc="~/Library/R/3.2/library")
api_key <- "NJNM2A4auNmoIQu0E4GtGGsJz"
api_secret <- "m7LrVXVMjsIG2lxmqj6zKreDypb1UuIph0x2vxikMslelda9K4"
access_token <- "785507712-aXrbfvmCAZnpXudDlbebcYAit9wXRS0jNZoYeOha"
access_token_secret <- "b2dpjiBWg36wkSCMLqMwfSJYRIk1rVtblqneU7tl4ECwg"
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)
library(plyr)
feed_sbcfiredispatch = laply(tweets_sbcfiredispatch, function(t) t$getText())
tweets_sbcf <- searchTwitter('@sbcfiredispatch', n=1500)
test <- as.data.frame(tweets_sbcf)
retryOnRateLimit
tweets_sbcf <- userTimeline('@sbcfiredispatch', n=1500)
as.data.frame(tweets_sbcf)
tweets_sbcf <- twListToDF(tweets_sbcf)
View(tweets_sbcf)
tweets_sbcf <- userTimeline('@sbcfiredispatch', n=3200)
min(tweets_sbcf$id)
tweets_sbcf$id[3200,]
tweets_sbcf[3200,]$id
library(twitteR)
tweets_sbcf <- twListToDF(tweets_sbcf)
tweets_sbcb$id
tweets_sbcf$id
tweets_sbcf$id[3200]
tweets_sbcf <- userTimeline('@sbcfiredispatch', n=3200, maxID = tweets_sbcf$id[3200])
tweets_sbcf <- twListToDF(tweets_sbcf)
View(tweets_sbcf)
tweets_sbcf_1 <- userTimeline('@sbcfiredispatch', n=3200)
tweets_sbcf_1 <- twListToDF(tweets_sbcf)
tweets_sbcf_1 <- twListToDF(tweets_sbcf_1)
View(tweets_sbcf_1)
tweets_sbcf_2 <- userTimeline('@sbcfiredispatch', n=3200, maxID = 'tweets_sbcf$id[3200]')
tweets_sbcf_2 <- twListToDF(tweets_sbcf_2)
tweets_sbcf_2 <- userTimeline('@sbcfiredispatch', n=3200, maxID = '768986797443604480')
tweets_sbcf_2 <- userTimeline('@sbcfiredispatch', n=3200, maxID = 768986797443604480)
tweets_sbcf_2 <- twListToDF(tweets_sbcf_2)
View(tweets_sbcf_2)
tweets_sbcf_2 <- userTimeline('@sbcfiredispatch', n=3000, maxID = 768986797443604480)
tweets_sbcf_2 <- twListToDF(tweets_sbcf_2)
View(tweets_sbcf_1)
tweets_sbcf_2 <- userTimeline('@sbcfiredispatch', n=3200, maxID = 768986797443604480)
tweets_sbcf_2 <- twListToDF(tweets_sbcf_2)
tweets_sbcf_2 <- userTimeline('@sbcfiredispatch', n=100, maxID = 768986797443604480)
tweets_sbcf_2 <- twListToDF(tweets_sbcf_2)
View(tweets_sbcf_1)
View(tweets_sbcf_2)
tweets_sbcf_3 <- userTimeline('@sbcfiredispatch', n=3200, maxID = 768897844032012288)
tweets_sbcf_3 <- twListToDF(tweets_sbcf_3)
View(tweets_sbcf_3)
View(tweets_sbcf_2)
will <- c("Will", "is", "dumb")
for (i in 1:3){
print(will[i])
}
library("class", lib.loc="~/Library/R/3.2/library")
methods(knn)
getAnywhere(knn)
getAnywhere(lm)
install.package("mlr")
install.packages('mlr')
library("mlr", lib.loc="~/Library/R/3.2/library")
install.packages("caret")
names(getModelInfo())
??names(getModelInfo())
library("caret", lib.loc="~/Library/R/3.2/library")
names(getModelInfo())
data <- iris
View(data)
train <- data[,1:4]
test <- data[-train]
test <- data[,-train]
test <- data[,5]
test <- as.df(data[,5])
test <- as.data.frame(data[,5])
names(getModelInfo())
names(getModelInfo())[1]
length(names(getModelInfo()))
for (i in 1:length(names(getModelInfo()))) {
model.[i] <- train(train, test, paste(i))
}
for (i in 1:length(names(getModelInfo()))) {
model.[i] <- train(train, test, paste(names(getModelInfo())[i]))
}
for (i in 1:length(names(getModelInfo()))) {
model.[i] <- train(train, test, paste(names(getModelInfo())[i]))
}
test <- data[,5]
for (i in 1:length(names(getModelInfo()))) {
model.[i] <- train(train, test, paste(names(getModelInfo())[i]))
}
warnings()
model <- train(train, test, 'ranger')
model <- train(train, test, 'ranger')
summary(model)
library(dplyr)
library(readr)
library(tidyr)
#Set WD to the Local Github Repo
setwd("~/Documents/Github/twitter-project")
#Load and Subset Data
tweets <- read.csv("tweets.csv")
tweets <- tweets[, c(2, 6)]
View(tweets)
library(dplyr)
library(readr)
library(tidyr)
#Set WD to the Local Github Repo
setwd("~/Documents/Github/twitter-project")
#Load and Subset Data
tweets <- read.csv("tweets.csv")
tweets <- tweets[, c(2, 6)]
View(tweets)
tweets <- tweets %>% separate(text, into = c("street", "other"), sep = "[\\,]", extra = "merge")
tweets <- tweets %>% separate('other', c("city", "emergency"), sep = "[\\*]", extra = "merge")
tweets <- tweets %>% separate('emergency', c("emergency", "other"), sep = "[\\*]", extra = "merge")
tweets <- tweets %>% separate('created', c("date", "time"), sep = 11, extra = "merge")
tweets <- tweets %>% separate('time', c("hour", "minute"), sep = 2, extra = "merge")
tweets$day <- as.Date(tweets$date) %>% weekdays()
View(tweets)
tweets <- data.frame(tweets$city, tweets$day, tweets$hour, tweets$emergency)
View(tweets)
cleanTweets <- function(tweets){
tweets <- tweets %>% separate(text, into = c("street", "other"), sep = "[\\,]", extra = "merge")
tweets <- tweets %>% separate('other', c("city", "emergency"), sep = "[\\*]", extra = "merge")
tweets <- tweets %>% separate('emergency', c("emergency", "other"), sep = "[\\*]", extra = "merge")
tweets <- tweets %>% separate('created', c("date", "time"), sep = 11, extra = "merge")
tweets <- tweets %>% separate('time', c("hour", "minute"), sep = 2, extra = "merge")
tweets$day <- as.Date(tweets$date) %>% weekdays()
tweets <- data.frame("city" = tweets$city,
"day" = tweets$day,
"hour" =tweets$hour,
"emergency" = tweets$emergency)
return(tweets)
}
tweets <- cleanTweets(tweets)
setwd("~/Documents/Github/twitter-project")
#Load and Subset Data
tweets <- read.csv("tweets.csv")
tweets <- tweets[, c(2, 6)]
#Use Regular expressions to split up fields
cleanSBTweets <- function(twitter){
twitter <- twitter %>% separate(text, into = c("street", "other"), sep = "[\\,]", extra = "merge")
twitter <- twitter %>% separate('other', c("city", "emergency"), sep = "[\\*]", extra = "merge")
twitter <- twitter  %>% separate('emergency', c("emergency", "other"), sep = "[\\*]", extra = "merge")
twitter <- twitter %>% separate('created', c("date", "time"), sep = 11, extra = "merge")
twitter <- twitter %>% separate('time', c("hour", "minute"), sep = 2, extra = "merge")
twitter$day <- as.Date(twitter$date) %>% weekdays()
twitter <- data.frame("city" = twitter$city,
"day" = twitter$day,
"hour" =twitter$hour,
"emergency" = twitter$emergency)
return(twitter)
}
tweets <- cleanSBTweets(tweets)
View(tweets)
tweets <- as.factor(tweets)
for (i in 1:ncol(tweets)){
tweets[i] <- as.factor(tweets[i])
}
View(tweets)
for (i in 1:ncol(tweets)){
tweets[i] <- as.factor(tweets[i])
}
tweets <- tweets[!apply(is.na(tweets) | tweets == "", 1, all),]
tweets <- tweets[!apply(is.na(tweets) | tweets == "", 1, all),]
View(tweets)
tweets[tweets==""]<-NA
View(tweets)
tweets[tweets == ""] <- NA
View(tweets)
tweets[tweets$city == "" | tweets$day == "" | tweets$hour == "" |tweets$emergency == ""] <- NA
View(tweets)
complete.cases(tweets)
tweets <- tweets[!complete.cases(tweets),]
View(tweets)
#Load and Subset Data
tweets <- read.csv("tweets.csv")
tweets <- tweets[, c(2, 6)]
#Use Regular expressions to split up fields
cleanSBTweets <- function(twitter){
twitter <- twitter %>% separate(text, into = c("street", "other"), sep = "[\\,]", extra = "merge")
twitter <- twitter %>% separate('other', c("city", "emergency"), sep = "[\\*]", extra = "merge")
twitter <- twitter  %>% separate('emergency', c("emergency", "other"), sep = "[\\*]", extra = "merge")
twitter <- twitter %>% separate('created', c("date", "time"), sep = 11, extra = "merge")
twitter <- twitter %>% separate('time', c("hour", "minute"), sep = 2, extra = "merge")
twitter$day <- as.Date(twitter$date) %>% weekdays()
twitter <- data.frame("city" = twitter$city,
"day" = twitter$day,
"hour" =twitter$hour,
"emergency" = twitter$emergency)
return(twitter)
}
tweets <- cleanSBTweets(tweets)
tweets <- tweets[complete.cases(tweets),]
View(tweets)
tweets <- tweets[!c(578, 1808, 2597, 1537),]
library(dplyr)
library(readr)
library(tidyr)
#Set WD to the Local Github Repo
setwd("~/Documents/Github/twitter-project")
#Load and Subset Data
tweets <- read.csv("tweets.csv")
tweets <- tweets[, c(2, 6)]
#Use Regular expressions to split up fields
cleanSBTweets <- function(twitter){
twitter <- twitter %>% separate(text, into = c("street", "other"), sep = "[\\,]", extra = "merge")
twitter <- twitter %>% separate('other', c("city", "emergency"), sep = "[\\*]", extra = "merge")
twitter <- twitter  %>% separate('emergency', c("emergency", "other"), sep = "[\\*]", extra = "merge")
twitter <- twitter %>% separate('created', c("date", "time"), sep = 11, extra = "merge")
twitter <- twitter %>% separate('time', c("hour", "minute"), sep = 2, extra = "merge")
twitter$day <- as.Date(twitter$date) %>% weekdays()
twitter <- data.frame("city" = twitter$city,
"day" = twitter$day,
"hour" =twitter$hour,
"emergency" = twitter$emergency)
return(twitter)
}
tweets <- cleanSBTweets(tweets)
tweets <- tweets[!c(578, 1808, 2597, 1537),]
View(tweets)
library(dplyr)
library(readr)
library(tidyr)
#Set WD to the Local Github Repo
setwd("~/Documents/Github/twitter-project")
#Load and Subset Data
tweets <- read.csv("tweets.csv")
tweets <- tweets[, c(2, 6)]
#Use Regular expressions to split up fields
cleanSBTweets <- function(twitter){
twitter <- twitter %>% separate(text, into = c("street", "other"), sep = "[\\,]", extra = "merge")
twitter <- twitter %>% separate('other', c("city", "emergency"), sep = "[\\*]", extra = "merge")
twitter <- twitter  %>% separate('emergency', c("emergency", "other"), sep = "[\\*]", extra = "merge")
twitter <- twitter %>% separate('created', c("date", "time"), sep = 11, extra = "merge")
twitter <- twitter %>% separate('time', c("hour", "minute"), sep = 2, extra = "merge")
twitter$day <- as.Date(twitter$date) %>% weekdays()
twitter <- data.frame("city" = twitter$city,
"day" = twitter$day,
"hour" =twitter$hour,
"emergency" = twitter$emergency)
return(twitter)
}
tweets <- cleanSBTweets(tweets)
#Delete Rows with NA's or missing values
tweets <- tweets[complete.cases(tweets),]
tweets <- tweets[-c(578, 1808, 2597, 1537),]
View(tweets)
for (i in 1:ncol(tweets)){
tweets[i] <- as.factor(tweets[i])
}
View(tweets)
summary(tweets$emergency)
tweets <- tweets[-c(578, 1808, 2597, 1537),]
summary(tweets$emergency)
